{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '9994'\n",
    "os.environ['RANK'] = \"0\"\n",
    "os.environ['LOCAL_RANK'] = \"0\"\n",
    "os.environ['WORLD_SIZE'] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"DEEPSPEED_ENABLE_PROFILING\"] = \"1\"\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, Subset, random_split\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForCausalLM, IntervalStrategy\n",
    "\n",
    "# Dataset Class\n",
    "class ExampleDataset(Dataset):\n",
    "    def __init__(self, argument_list, example_list, topic_list, disctype_list, tokenizer, max_length):\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        for argument, example, topic, disctype in zip(argument_list, example_list, topic_list, disctype_list):\n",
    "            prep_argument = (f'<|startoftext|>For an essay on the topic {topic}, '\n",
    "                             f'give a better example for this ineffective {disctype}'\n",
    "                             f' : {argument}\\n<|sep|>Better example : {example}<|endoftext|>')\n",
    "            #prep_argument = f'<|startoftext|>Argument: {argument}\\nRewrite a more effective version: {example}<|endoftext|>'\n",
    "            # tokenize \n",
    "            encodings_dict = tokenizer(prep_argument, \n",
    "                                       truncation=True,\n",
    "                                       max_length = max_length, \n",
    "                                       padding=\"max_length\")\n",
    "            # append to list\n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "\n",
    "            # when training, the input data will be passed in also as the label \n",
    "            # because we are training a language model and we want the model to\n",
    "            # learn the pattern of argument + example struture\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx]\n",
    "    \n",
    "def load_dataset(tokenizer):\n",
    "    # load dataset\n",
    "    filepath = \"data/effective/dataset_with_best_example_and_topic.csv\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    df = df.sample(1000).reset_index()\n",
    "    max_length = max([len(tokenizer.encode(text)) for text in df['discourse_text']])\n",
    "    print(\"Max length: {}\".format(max_length))\n",
    "    \n",
    "    # split \n",
    "    n = len(df)\n",
    "    n_train = int(0.99 * n)\n",
    "    indices = list(range(n))\n",
    "    random.shuffle(indices)\n",
    "    train_args = Subset(df['discourse_text'], indices[:n_train])\n",
    "    val_args = Subset(df['discourse_text'], indices[n_train:])\n",
    "    train_exps = Subset(df['predictions'], indices[:n_train])\n",
    "    val_exps = Subset(df['predictions'], indices[n_train:])\n",
    "    train_tpcs = Subset(df['topics'], indices[:n_train])\n",
    "    val_tpcs = Subset(df['topics'], indices[n_train:])\n",
    "    train_typs = Subset(df['discourse_type'], indices[:n_train])\n",
    "    val_typs = Subset(df['discourse_type'], indices[n_train:])\n",
    "\n",
    "     # generate class\n",
    "    train_dataset = ExampleDataset(train_args, train_exps, train_tpcs, train_typs,\n",
    "                                   tokenizer, max_length=max_length*2)\n",
    "    \n",
    "    return train_dataset, (val_args, val_exps, val_tpcs, val_typs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model_name = \"gpt2\"\n",
    "#model_name = \"EleutherAI/gpt-neo-2.7B\"\n",
    "#special_tokens_dict = {'eos_token': eos, 'bos_token': bos, 'pad_token': pad}\n",
    "#tokenizer_orig.add_special_tokens(special_tokens_dict)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, \n",
    "                                          bos_token='<|startoftext|>', \n",
    "                                          eos_token='<|endoftext|>', \n",
    "                                          sep_token='<|sep|>',\n",
    "                                          pad_token='<pad>')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).cuda()\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "train_dataset, val_dataset = load_dataset(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train\n",
    "\n",
    "training_args = TrainingArguments(output_dir='./results', \n",
    "                                  num_train_epochs=5, \n",
    "                                  logging_steps=500, \n",
    "                                  save_strategy=IntervalStrategy.NO,\n",
    "                                  per_device_train_batch_size=2, \n",
    "                                  per_device_eval_batch_size=2, \n",
    "                                  warmup_steps=100,\n",
    "                                  weight_decay=0.01, \n",
    "                                  logging_dir='./logs', \n",
    "                                  fp16=True, \n",
    "                                  deepspeed='./ds_config.json')\n",
    "\n",
    "trainer = Trainer(model=model, \n",
    "        args=training_args, \n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset, \n",
    "        data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n",
    "                                    'attention_mask': torch.stack([f[1] for f in data]),\n",
    "                                    'labels': torch.stack([f[0] for f in data])})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"start training\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"./models/gpt2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval\n",
    "\n",
    "print(\"start evaluating\")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"./models/\")\n",
    "\n",
    "results = dict()\n",
    "idx = 0\n",
    "\n",
    "for argument, example, topic, disctype in tqdm(zip(val_dataset[0], val_dataset[1], val_dataset[2], val_dataset[3])):\n",
    "    #prepare promp\n",
    "    prep_argument = (f'<|startoftext|>For an essay on the topic {topic}, '\n",
    "                        f'give a better example for this ineffective {disctype}'\n",
    "                        f' : {argument}\\n<|sep|>Better example : ')\n",
    "    generated = tokenizer(prep_argument, \n",
    "                      return_tensors=\"pt\").input_ids.cuda()\n",
    "    #generate\n",
    "    sample_outputs = model.generate(generated, \n",
    "                                    do_sample=True, \n",
    "                                    top_k=50,\n",
    "                                    bos_token='<|startoftext|>',\n",
    "                                    eos_token='<|endoftext|>',\n",
    "                                    sep_token='<|sep|>',\n",
    "                                    pad_token='<pad>',\n",
    "                                    max_length=len(argument), \n",
    "                                    top_p=0.95, \n",
    "                                    temperature=1.9, \n",
    "                                    num_return_sequences=20)\n",
    "\n",
    "    pred = tokenizer.decode(sample_outputs[0], skip_special_tokens=True)\n",
    "    results[idx] = {'input': argument, \n",
    "                    'pred': pred,\n",
    "                    'true': example}\n",
    "    idx += 1\n",
    "\n",
    "json_output = json.dumps(results, indent=4) \n",
    "with open(\"data/effective/finetune_gpt2_example.json\", \"w\") as outfile:\n",
    "        outfile.write(json_output)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
